{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce20f341-61b8-4c5d-a957-001b6ac9ed3c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /Users/shanecupid/anaconda3/lib/python3.11/site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/shanecupid/anaconda3/lib/python3.11/site-packages (from beautifulsoup4) (2.4)\n",
      "Requirement already satisfied: requests in /Users/shanecupid/anaconda3/lib/python3.11/site-packages (2.29.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/shanecupid/anaconda3/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shanecupid/anaconda3/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/shanecupid/anaconda3/lib/python3.11/site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shanecupid/anaconda3/lib/python3.11/site-packages (from requests) (2023.5.7)\n",
      "Requirement already satisfied: pandas in /Users/shanecupid/anaconda3/lib/python3.11/site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/shanecupid/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/shanecupid/anaconda3/lib/python3.11/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/shanecupid/anaconda3/lib/python3.11/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/shanecupid/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: lxml in /Users/shanecupid/anaconda3/lib/python3.11/site-packages (4.9.2)\n",
      "Requirement already satisfied: matplotlib in /Users/shanecupid/anaconda3/lib/python3.11/site-packages (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/shanecupid/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/shanecupid/anaconda3/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/shanecupid/anaconda3/lib/python3.11/site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/shanecupid/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/shanecupid/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/shanecupid/anaconda3/lib/python3.11/site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/shanecupid/anaconda3/lib/python3.11/site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/shanecupid/anaconda3/lib/python3.11/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/shanecupid/anaconda3/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/shanecupid/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: numpy in /Users/shanecupid/anaconda3/lib/python3.11/site-packages (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "#Installing Libraries\n",
    "!pip install beautifulsoup4\n",
    "!pip install requests\n",
    "!pip install pandas\n",
    "!pip install lxml\n",
    "!pip install matplotlib\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11e6cd7d-262d-4906-9570-9ecdb248ba00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 'requests' library is used to make HTTP requests to a web page, allowing you to retrieve the HTML content of the page.\n",
    "import requests\n",
    "\n",
    "# 'BeautifulSoup' from the 'bs4' library is used to parse the HTML content retrieved from a web page. It provides methods to search and navigate the HTML tree, making it easier to extract the data you need.\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 'pandas' is a powerful library for data manipulation and analysis. It's commonly used in web scraping to store and manipulate the scraped data in a tabular format (DataFrame), making it easier to analyze, filter, and export the data.\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11b0aca4-1962-4cf4-87c0-a8819cee69f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# List of URLs for the last 5 years\n",
    "urls = [\n",
    "    'http://pacificpuddlejump.com/fleet.html', # 2023\n",
    "    'http://pacificpuddlejump.com/Alumni/2022fleet.html', # 2022 \n",
    "    'http://pacificpuddlejump.com/Alumni/2019fleet.html', # 2019\n",
    "    'http://www.pacificpuddlejump.com/Alumni/2018fleet.html', # 2018\n",
    "    'http://www.pacificpuddlejump.com/Alumni/2017fleet.html', # 2017\n",
    "]\n",
    "\n",
    "# Iterate through the URLs and perform the scraping process\n",
    "for url in urls:\n",
    "    page = requests.get(url) # allows you to fetch the content of each URL one by one\n",
    "    if page.status_code == 200: # If we get the response 200 then the server allows us to collect data from the sites. \n",
    "        # Extract the data you need\n",
    "        # Store the data in a structured format\n",
    "        print(\"Success\")\n",
    "    else:\n",
    "        print(\"Failed to fetch data from \" + url) # added else statement to identify if there was an invalid URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c205d8a1-7b96-4e03-9070-c07251e08aa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_year(url):  # Function to extract the year from the URL\n",
    "    year_part = url.split('/')[-1][:4] #extracts the year. It splits the URL string into a list using / then \"[-1] takes that last element. - the year. The \"4\" takes the first 4 char of that element which is the year\n",
    "    return '2023' if year_part == 'flee' else year_part # the first four character is flee for the first link, therfore the year is 2023\n",
    "\n",
    "def process_row(row):  # Function to process a single row of the table\n",
    "    return [cell.text.strip() for cell in row.find_all('td')] #find all gives that list of td elements.cell.text extracts the text content between the cell (opening and closing tags). Strip() just removes any white spaces\n",
    "\n",
    "def scrape_url(url):  # Function to handle the scraping of a single URL\n",
    "    page = requests.get(url) #sends that HTTP GET request to the URL then the repsonse is stored in the 'page' variable. Its the raw HTML content that has yet to be parsed for easy retrieval of data\n",
    "    soup = BeautifulSoup(page.text, 'lxml') #parses the HTML content of the page to make it easier to navigate in a more pythonic/lxml way. page.text grabs all HTML content in a single string\n",
    "    year = get_year(url)\n",
    "    table = soup.find('table', id='fleet') #locates the table(s) where extracting from\n",
    "    return [[*process_row(row), year] for row in table.find_all('tr')[1:]]\n",
    "            #Calls the previously defined function and extracts the cell data\n",
    "            #, year appends year.. it adds year to the end of the list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "066af3fb-8c78-41b6-b14b-2c9214b99fa9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Boat Model                Homeport  \\\n",
      "Boat Name                                                  \n",
      "Aloha Toa        Wharram Tiki 30                  La Paz   \n",
      "Ana Maria    Pacific Seacraft 34             Seattle, WA   \n",
      "Arctic Tern     Island Packet 40              Kailua, HI   \n",
      "Beleza                  Hylas 46           Hermosa Beach   \n",
      "Best Life             Leopard 47      Salt Lake City, UT   \n",
      "...                          ...                     ...   \n",
      "Windsong                   CT 47            Portland, OR   \n",
      "Wiz                      Swan 44             Seattle, WA   \n",
      "Xirene            X-Yachts X-482         Muiderzand, NED   \n",
      "Y2K          Beneteau Oceanis 50  Marina di Carrara, ITA   \n",
      "Zatara       Beneteau Oceanis 55            Montana, USA   \n",
      "\n",
      "                            Owner's Name   Departing From            When  \\\n",
      "Boat Name                                                                   \n",
      "Aloha Toa               Tymoteo Shaddock             Baja         April 5   \n",
      "Ana Maria    Andres & Katherine Gonzalez           La Paz        February   \n",
      "Arctic Tern         Shaen & Laura Tarter     Honolulu, HI        April 15   \n",
      "Beleza                 Christopher Maler   Cabo San Lucas  March or April   \n",
      "Best Life              Benjamin Fletcher  Puerto Vallarta         March 8   \n",
      "...                                  ...              ...             ...   \n",
      "Windsong             John & Janice Barba        Galapagos        April 21   \n",
      "Wiz                Darrell & Susan Clark          La Cruz           April   \n",
      "Xirene                    David de Graaf          Curacao      February 1   \n",
      "Y2K                         Max Terragni    Balboa Panama       Mid-March   \n",
      "Zatara                    Keith Whitaker        Galapagos        April 15   \n",
      "\n",
      "             Year  \n",
      "Boat Name          \n",
      "Aloha Toa    2023  \n",
      "Ana Maria    2023  \n",
      "Arctic Tern  2023  \n",
      "Beleza       2023  \n",
      "Best Life    2023  \n",
      "...           ...  \n",
      "Windsong     2017  \n",
      "Wiz          2017  \n",
      "Xirene       2017  \n",
      "Y2K          2017  \n",
      "Zatara       2017  \n",
      "\n",
      "[641 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "final_data = []  # Initialize an empty list to store the final data. Starts enmpty since its outside the loop but will later be filled once the loop starts itetating through the urls\n",
    "\n",
    "for url in urls:  # Process each URL using the 'scrape_url' function\n",
    "    final_data.extend(scrape_url(url)) #Unlike the append method, which would add the entire list as a single element, extend adds each element of the list separately\n",
    "\n",
    "headers = ['Boat Name', 'Boat Model', 'Homeport', \"Owner's Name\", 'Departing From', 'When', 'Year']  # Define column headers\n",
    "\n",
    "df = pd.DataFrame(final_data, columns=headers)  # Create DataFrame using combined data. columns=headers(list contains the name of the columns)Telling pandas to use these names for the columns. It aligns the data from final_data to each of these columns\n",
    "#df is just the varibale with the now structured dataframe object\n",
    "df.set_index(\"Boat Name\", inplace=True)  # Set index(pandas dataframe) to 'Boat Name' inplace=True tells pandas to modify the existing dataframe otherwise a new one will be created\n",
    "\n",
    "print(df)  # Display the final DataFrame\n",
    "#PANDAS.APPLY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c341d7-a362-4261-a801-cf700b6257d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "This code block performs the following tasks to scrape fleet information from five different URLs:\n",
    "1.\tInitialization: Creates an empty list final_data to store the scraped data.\n",
    "2.\tURL Iteration: Iterates through the provided URLs, sending a GET request and parsing the HTML content.\n",
    "3.\tYear Extraction: Extracts the year from the URL.\n",
    "4.\tTable Identification: Finds the table containing the fleet information.\n",
    "5.\tRow Iteration: Iterates through the table rows, extracting text from each cell and appending it to final_data along with the year.\n",
    "6.\tDataFrame Creation: Constructs a DataFrame from the combined data with specific column headers.\n",
    "7.\tYear Replacement: Replaces the value 'flee' with '2023' in the 'Year' column.\n",
    "8.\tIndex Setting: Sets the DataFrame index to 'Boat Name'.\n",
    "9.\tOutput: Prints the DataFrame, showing the combined data from all five tables, including the additional year column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cfe4754-52e0-49f1-9148-570dfabd464d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('combinedSauce.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3c3082-94d1-4b11-833d-6f981aa828ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
